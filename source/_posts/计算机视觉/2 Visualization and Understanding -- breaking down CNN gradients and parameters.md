---
categories:
  - 计算机视觉
date: '2025-11-27 13:31:46'
title: >-
  2 Visualization and Understanding -- breaking down CNN gradients and
  parameters
---
在 CNN 取得巨大成功后，一个核心问题随之而来：**“神经网络到底学到了什么？”** 我们常常把深度网络称为“黑盒”，而这一章的内容就是试图打开这个黑盒，通过可视化手段来解释网络的内部运作机制。

---

### 第一部分：可视化网络内部 (Visualizing Weights & Activations)

我们要看网络内部到底长什么样，最直接的方法就是看它的**权重（Weights）** 和**激活值（Activations）**。

#### 1. 可视化第一层卷积核 (Visualizing First Layer Filters)
* **现象**：当我们把 CNN 第一层卷积核（例如 $64 \times 3 \times 7 \times 7$）可视化出来时，会发现它们惊人地相似，无论是在 AlexNet、ResNet 还是其他网络中 。
* **解释**：第一层卷积核主要学到的是**边缘（Edges）** 和**颜色斑点（Blobs）**。这是因为图像的局部统计特性决定了边缘是构成视觉世界的最基本元素。这与人类视觉皮层 V1 区的感受野非常相似。
* **局限**：直接可视化权重只对第一层有效。更高层的卷积核处理的是上一层的特征图，维度更高且抽象，直接看权重已经无法理解其几何意义。

#### 2. 可视化激活图 (Visualizing Activations)
既然看权重不行，那就看**激活值**，也就是看“当网络看到某张图时，哪些神经元亮了”。
* **特征图（Feature Maps）**：可视化某一层的输出特征图。你会发现，浅层的特征图保留了较多空间细节，而深层的特征图变得更加**稀疏（Sparse）** 和**语义化**。有些特征图可能专门对“人脸”有反应，有些可能对“文字”有反应 。
* **最大激活补丁 (Maximally Activating Patches)**：
    * **方法**：选取某个特定的神经元（比如第 5 层的第 17 个通道），遍历整个数据集，找出能让这个神经元产生最大激活值的那些图片区域（Patches）。
    * **发现**：你会发现某些神经元是“纹理检测器”（如专门检测圆点），有些是“物体部件检测器”（如专门检测狗头、眼睛或车轮）这证明了 CNN 确实在进行层级化的特征提取。

#### 3. 降维可视化 (Dimensionality Reduction)
* **t-SNE**：CNN 的倒数第二层通常是一个高维向量（如 4096 维），代表了图像的“语义编码”。我们可以用 t-SNE 算法将其降维到 2D 平面进行散点图可视化。
* **结果**：你会发现，语义相似的图片（比如不同姿态的狗、不同背景的船）在降维后的空间中聚集在一起。这说明 CNN 学到了**语义距离**，而不仅仅是像素距离 。

---

### 第二部分：理解决策依据 (Understanding Input Pixels)

这部分回答的问题是：**“网络为什么觉得这是一只大象？”** 或者是 **“图片中的哪些像素导致了分类结果？”**

#### 1. 遮挡实验 (Occlusion Experiments)
* **原理**：如果我在图片上放一个灰色的方块遮挡住某个部位，网络对正确类别的预测概率大幅下降，说明这个部位对分类至关重要 。
* **热力图**：通过滑动遮挡窗口，我们可以绘制出一个“重要性热力图”。

#### 2. 显著性图 (Saliency Maps)
* **基于梯度的方法**：我们可以计算类别分数 $S_c$ 相对于输入图像像素 $I$ 的梯度 $\frac{\partial S_c}{\partial I}$。
* **直觉**：梯度告诉我们，如果我们稍微改变某个像素的值，类别分数会发生多大变化。梯度大的像素就是“关键像素”。
* **效果**：这可以勾勒出物体的轮廓，告诉我们网络关注的是物体的形状。

#### 3. 类激活映射 (CAM & Grad-CAM)
这是一个非常经典且强大的技术，用于**弱监督定位**。
* **CAM (Class Activation Mapping)**：针对使用全局平均池化（GAP）的网络。它利用 GAP 层之前的特征图，加权叠加得到热力图。
* **Grad-CAM**：CAM 的通用推广版。它不需要修改网络结构。
    * **原理**：计算目标类别对最后一个卷积层特征图的梯度，对梯度进行全局平均池化得到“权重”。这些权重表示了每个特征图对该类别的“贡献度”。
    * **公式**：$L_{Grad-CAM}^c = ReLU(\sum_k \alpha_k^c A^k)$。
    * **结果**：Grad-CAM 能在原图上生成一个粗略但准确的热力图，高亮显示出网络“看”哪里。比如从一张“猫狗合照”中，Grad-CAM 能准确地在预测“猫”时高亮猫的区域，预测“狗”时高亮狗的区域。

---

### 第三部分：基于优化的可视化与生成 (Visualization with Optimization)

这部分不再是“看”网络，而是利用网络来“创造”图像。其核心思想是：**固定网络参数，通过梯度下降优化输入像素**。

#### 1. 类别可视化 (Class Visualization / Model Inversion)
* **目标**：我们也想知道，在网络心目中，完美的“火烈鸟”长什么样？
* **方法**：随机初始化一张噪声图像，固定网络参数，定义损失函数为“火烈鸟类别的得分”。通过反向传播计算**对图像像素的梯度**，不断更新图像像素，使网络认为它越来越像火烈鸟。
* **正则化**：直接优化会得到充满高频噪声的怪异图像。因此必须加入正则化项（如 L2 正则、高斯模糊等），强制生成的图像保持平滑自然。

#### 2. DeepDream
* **原理**：这是 Google 提出的一个有趣实验。它选取网络中的某一层，将该层的激活值作为目标进行最大化。
* **过程**：输入一张图，前向传播，放大某些特征的激活值，然后反向传播修改原图。
* **效果**：网络会把它在图像中“隐约”看到的模式（比如云朵像狗）强行放大和加深，最终生成充满幻觉般的、充满狗头和眼睛的迷幻图像。

#### 3. 神经风格迁移 (Neural Style Transfer)
这是可视化技术最著名的应用之一（例如 Prisma 应用）。
* **任务**：给定一张**内容图（Content）**（如你的照片）和一张**风格图（Style）**（如《星月夜》），生成一张新图，既保留内容又拥有该风格。
* **损失函数设计**：
    * **内容损失 (Content Loss)**：利用 CNN 深层特征（如 VGG 的 conv4_2）的欧氏距离。深层特征捕捉物体结构，忽略像素细节。
    * **风格损失 (Style Loss)**：利用**格拉姆矩阵 (Gram Matrix)**。格拉姆矩阵计算的是特征图之间的**相关性（Correlation）**。这种统计特性（如“这种纹理通常和那种颜色一起出现”）捕捉了画面的笔触和纹理风格，而丢弃了空间位置信息。
* **优化**：总损失 = $\alpha \times$ 内容损失 + $\beta \times$ 风格损失。通过优化输入噪声图像来最小化总损失。

#### 4. 快速风格迁移 (Fast Neural Style Transfer)
* 传统的风格迁移需要对每张图进行在线优化，速度很慢。
* **改进**：训练一个前馈网络（Feed-forward Network）来直接生成风格化图像。训练时利用上述的 Perceptual Loss（感知损失），推理时只需一次前向传播，实现了实时风格迁移。
* **Instance Normalization**：PPT 提到一个细节，风格迁移网络中，使用 Instance Normalization（对单张图片做归一化）比 Batch Normalization 效果好得多，因为它保留了单张图片的独特的风格统计特性。

---

### 总结

这一章的核心在于**利用梯度（Gradient）的双向流动**：
1.  **训练时**：梯度对**权重**求导，更新权重，让网络学习。
2.  **可视化时**：梯度对**输入像素**求导，更新图像，让我们理解网络想要看什么，或者生成我们想要的艺术图像。

这些技术不仅让我们对 CNN 更加信任（Explainable AI），也开启了 AI 生成艺术的大门。